import requests
from bs4 import BeautifulSoup
import sqlite3
import time
import random
from datetime import datetime
import logging
import re

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FinanceNewsCrawler:
    def __init__(self):
        # è´¢ç»æ–°é—»ç½‘ç«™URL (åªä¿ç•™ä¸œæ–¹è´¢å¯Œ)
        self.urls = [
            'https://finance.eastmoney.com/' # ä¸œæ–¹è´¢å¯Œ
        ]
        
        # è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        try:
            conn = sqlite3.connect('eastmoney_hot_news.db', check_same_thread=False)
            cursor = conn.cursor()
            
            cursor.execute(''' 
                CREATE TABLE IF NOT EXISTS news (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    title TEXT NOT NULL,
                    url TEXT UNIQUE NOT NULL,
                    source TEXT NOT NULL,
                    publish_time TEXT, -- è¿™é‡Œå­˜å‚¨ "2026-02-19 10:03 æ¥æºï¼šç•Œé¢æ–°é—»"
                    content TEXT,
                    summary TEXT,
                    category TEXT,
                    tags TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆ›å»ºç´¢å¼•
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_url ON news(url)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON news(source)')
            conn.commit()
            conn.close()
            logger.info("æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")

    def get_db_connection(self):
        """åˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®åº“è¿æ¥"""
        return sqlite3.connect('eastmoney_hot_news.db', check_same_thread=True)

    def get_page_content(self, url):
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.encoding = response.apparent_encoding
            if response.status_code == 200:
                return response.text
            else:
                logger.error(f"è¯·æ±‚å¤±è´¥: {response.status_code} - {url}")
                return None
        except Exception as e:
            logger.error(f"è¯·æ±‚å¼‚å¸¸: {e} - {url}")
            return None

    def parse_eastmoney(self, html):
        """è§£æä¸œæ–¹è´¢å¯Œï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        news_list = []
        if not html:
            return news_list
            
        soup = BeautifulSoup(html, 'html.parser')
        
        # ç­–ç•¥1: æŠ“å–ä¸»è¦æ–°é—»åˆ—è¡¨ (class="list" æˆ– "news-list")
        for ul_selector in ['ul.list', 'ul.news-list', '.leftContent ul']:
            ul_tags = soup.select(ul_selector)
            for ul in ul_tags:
                for li in ul.select('li'):
                    a_tag = li.select_one('a')
                    if a_tag and a_tag.get('href'):
                        href = a_tag['href']
                        title = a_tag.get_text().strip()
                        
                        # è¡¥å…¨URL
                        if href.startswith('//'):
                            href = 'https:' + href
                        elif href.startswith('/'):
                            href = 'https://finance.eastmoney.com' + href
                            
                        # è¿‡æ»¤æ— æ•ˆé“¾æ¥å’Œæ ‡é¢˜
                        if ('http' in href and 
                            len(title) > 5 and 
                            not title.startswith('æŸ¥çœ‹æ›´å¤š') and
                            'å¹¿å‘Š' not in title):
                            news_list.append({
                                'title': title,
                                'url': href,
                                'source': 'ä¸œæ–¹è´¢å¯Œ',
                                'category': 'è‚¡ç¥¨/åŸºé‡‘'
                            })

        # ç­–ç•¥2: å»é‡ (åŸºäºURL)
        seen = set()
        unique_news = []
        for item in news_list:
            if item['url'] not in seen:
                seen.add(item['url'])
                unique_news.append(item)
                
        logger.info(f"è§£æåˆ° {len(unique_news)} æ¡æ–°é—»")
        return unique_news

    def get_news_content(self, url):
        """è·å–æ–°é—»è¯¦ç»†å†…å®¹ï¼ˆå‡çº§ç‰ˆï¼šç²¾å‡†æå–æ—¶é—´ä¸æ¥æºï¼‰"""
        html = self.get_page_content(url)
        if not html:
            return None, None
        
        soup = BeautifulSoup(html, 'html.parser')
    
        # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
        for tag in soup(['script', 'style', 'nav', 'footer', 'aside']):
            tag.decompose()
        
        # æå–æ­£æ–‡ (ä¿æŒä¸å˜)
        content_selectors = [
            '.article-body', '.content', '.text', '#ContentBody',
            '.news-content', '.post-content', '.article-content'
        ]
        content = ""
        for sel in content_selectors:
            elements = soup.select(sel)
            for el in elements:
                text = el.get_text(strip=False)
                if len(text) > 100:
                    content = text
                    break
            if content:
                break

        # --- æ ¸å¿ƒå‡çº§ï¼šç²¾å‡†æå–æ—¶é—´ä¸æ¥æº ---
        # 1. æå–æ—¶é—´ (æ ¹æ®ä½ æä¾›çš„ç»“æ„ï¼šclass=" item" æˆ– "item")
        # ä½¿ç”¨æ­£åˆ™åŒ¹é… classï¼Œå…¼å®¹ " item" å’Œ "item"
        time_div = soup.find('div', class_=re.compile(r'\bitem\b'))
        time_text = ""
        if time_div:
            raw_time = time_div.get_text(strip=True)
            # ä½¿ç”¨æ­£åˆ™æå–æ ‡å‡†æ—¶é—´æ ¼å¼ (æ”¯æŒ "2026å¹´02æœˆ11æ—¥ 13:25")
            time_match = re.search(r'\d{4}å¹´\d{2}æœˆ\d{2}æ—¥\s+\d{2}:\d{2}', raw_time)
            if time_match:
                time_text = time_match.group(0) # æå–çº¯æ—¶é—´
    
        # 2. æå–æ¥æº (æ ¹æ®ä½ æä¾›çš„ç»“æ„ï¼šåŒ…å«â€œæ¥æºï¼šâ€çš„ div)
        # å¯»æ‰¾åŒ…å«â€œæ¥æºâ€æ–‡æœ¬çš„ div.item
        source_div = soup.find('div', class_='item', string=re.compile(r'æ¥æº'))
        source_text = ""
        if source_div:
            raw_source = source_div.get_text(strip=True)
            # ç¡®ä¿æ ¼å¼ä¸ºâ€œæ¥æºï¼šXXXâ€
            if 'æ¥æº' in raw_source:
                source_text = raw_source
            else:
            # å¦‚æœæ–‡æœ¬é‡Œæ²¡æœ‰â€œæ¥æºâ€äºŒå­—ä½†è¢«æŠ“åˆ°äº†ï¼ŒåŠ ä¸Šå‰ç¼€
                source_text = f"æ¥æºï¼š{raw_source}"

        # 3. æ‹¼æ¥ç»“æœ (ä¾‹å¦‚: "2026å¹´02æœˆ11æ—¥ 13:25 æ¥æºï¼šä¸œæ–¹è´¢å¯Œç½‘")
        # ä¼˜å…ˆä½¿ç”¨æå–åˆ°çš„æ—¶é—´å’Œæ¥æº
        if time_text and source_text:
            publish_time = f"{time_text} {source_text}"
        elif time_text:
            publish_time = time_text
        elif source_text:
            publish_time = source_text
        else:
            publish_time = "æœªçŸ¥æ—¶é—´/æ¥æº"
        
        # --- ç»“æŸå‡çº§ ---

        return content.strip(), publish_time

    def save_news(self, news_data):
        """ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“ï¼ˆæ¯æ¬¡æ–°å»ºè¿æ¥ï¼‰"""
        if not news_data:
            return False
            
        conn = None
        try:
            # 1. æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ (æ–°å»ºè¿æ¥)
            conn = self.get_db_connection()
            cursor = conn.cursor()
            cursor.execute('SELECT id FROM news WHERE url = ?', (news_data['url'],))
            if cursor.fetchone():
                logger.info(f"å·²å­˜åœ¨ï¼Œè·³è¿‡: {news_data['title']}")
                return False
                
            # 2. æ’å…¥æ–°æ•°æ®
            cursor.execute(''' 
                INSERT INTO news 
                (title, url, source, publish_time, content, summary, category, tags) 
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                news_data['title'],
                news_data['url'],
                news_data['source'],
                news_data.get('publish_time'), # è¿™é‡Œå­˜å…¥çš„æ˜¯æ‹¼æ¥åçš„å­—ç¬¦ä¸²
                news_data.get('content', '')[:5000],
                news_data.get('summary', '')[:200],
                news_data.get('category'),
                news_data.get('tags')
            ))
            conn.commit()
            logger.info(f"âœ… æˆåŠŸä¿å­˜: {news_data['title']}")
            return True
            
        except Exception as e:
            logger.error(f"ğŸ’¾ ä¿å­˜å¤±è´¥: {e}")
            return False
        finally:
            if conn:
                conn.close()

    def crawl(self):
        """æ‰§è¡Œçˆ¬å–ä»»åŠ¡"""
        logger.info("å¼€å§‹è´¢ç»æ–°é—»çˆ¬å–...")
        
        # ç¡®ä¿è¡¨å­˜åœ¨
        self.init_database()
        
        all_news = []
        for url in self.urls:
            logger.info(f"æ­£åœ¨çˆ¬å–: {url}")
            html = self.get_page_content(url)
            news_list = self.parse_eastmoney(html)
            all_news.extend(news_list)
            time.sleep(random.uniform(1, 2)) # éšæœºå»¶æ—¶

        logger.info(f"å…±å‘ç° {len(all_news)} æ¡æ–°é—»ï¼Œå¼€å§‹è·å–è¯¦æƒ…...")
        
        success_count = 0
        for news in all_news:
            content, publish_time = self.get_news_content(news['url'])
            news['content'] = content
            news['publish_time'] = publish_time # èµ‹å€¼ç»™ news_data
            news['summary'] = content[:200] + "..." if content and len(content) > 200 else (content or "æš‚æ— æ‘˜è¦")
            
            if self.save_news(news):
                success_count += 1
            time.sleep(random.uniform(0.1, 0.3))
            
        logger.info(f"çˆ¬å–ä»»åŠ¡å®Œæˆï¼ŒæˆåŠŸä¿å­˜ {success_count} æ¡æ–°é—»")
        return success_count

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    crawler = FinanceNewsCrawler()
    crawler.crawl()

# åœ¨ crawler.py çš„ crawl å‡½æ•°æœ€å
def crawl_for_api(self):
    # ... (å‰é¢çš„ä»£ç ä¸å˜) ...
    # æ‰¾åˆ° all_news = [] ... ç­‰é€»è¾‘
    # åœ¨æœ€åï¼š
    result = []
    for news in all_news:
        # æ„é€ ç¬¦åˆ API æ ¼å¼çš„æ•°æ®
        item = {
            "id": hash(news['url']) % 100000, # ç®€å•ç”Ÿæˆä¸€ä¸ª ID
            "title": news['title'],
            "url": news['url'],
            "source": news['source'],
            "publish_time": "æœªçŸ¥", # è¿™é‡Œéœ€è¦ä½ å®Œå–„
            "summary": news['title'][:50] + "..." # ç®€å•æˆªå–æ‘˜è¦
        }
        result.append(item)
    return result

